{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8efd43",
   "metadata": {},
   "source": [
    "# MSSE 277B: Machine Learning Algorithms \n",
    "\n",
    "## Homework assignment #7: Deep Learning and Regularization \n",
    "\n",
    "### Assigned March 9 and Due March 21 \n",
    "\n",
    "### Student Name: Charis Liao \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a9409ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3052a",
   "metadata": {},
   "source": [
    "# 1. Biad-varaince tradeoff \n",
    "We will use the MNIST data set to train, validate, and test using a deep learning network. The input representation will be raw image vector for each number comprised of 32x32 pixels(=1024 in length) with one channel of real values between 0 (black) and 255 (white). Use cross-entropy as the loss function.     \n",
    "\n",
    "**(a) normalize the entire data set by dividing each number image by its maximum pixel value for numerical stability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9915a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('mnist.pkl')\n",
    "\n",
    "# print(f'length of df: {len(df)}')\n",
    "# print(f'len df[0]: {len(df[0])}')\n",
    "# print(f'len df[1]: {len(df[1])}')\n",
    "# print(f'df[0][0]: {len(df[0][0])}')\n",
    "# print(f'df[0][1]: {len(df[0][1])}')\n",
    "# print(f'df[1][0]: {len(df[1][0])}')\n",
    "# print(f'df[1][1]: {len(df[1][1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de56abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the entire data set\n",
    "x_train = df[0][0] / 255.0\n",
    "y_train = df[0][1]\n",
    "x_test = df[1][0] / 255.0\n",
    "y_test = df[1][1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cbb33",
   "metadata": {},
   "source": [
    "**(b) The data folder you are given is already organized into a training set and a test set. Therefore, to create a validation set, divide the original training data into 3-fold groups of training (2/3 of the training data), validation (1/3 of the training data). 3-fold is because you will run 3 independent training/validation sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3213a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func:%r  took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12d83805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(complete_list, chunk_size=None, num_chunks=None):\n",
    "    '''\n",
    "    Cut a list into multiple chunks, each having chunk_size (the last chunk might be less than chunk_size) or having a total of num_chunk chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    if num_chunks is None:\n",
    "        num_chunks = math.ceil(len(complete_list) / chunk_size)\n",
    "    elif chunk_size is None:\n",
    "        chunk_size = math.ceil(len(complete_list) / num_chunks)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(complete_list[i * chunk_size: (i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x,):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = Adam(model.parameters(), learning_rate)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "\n",
    "    @timing\n",
    "    def train(self, inputs, outputs, val_inputs, val_outputs,early_stop=False,l2=False,silent=False):\n",
    "        \"\"\" train self.model with specified arguments\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_nputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \n",
    "        @return\n",
    "        a dictionary of arrays with train and val losses and accuracies\n",
    "        \"\"\"\n",
    "        ### convert data to tensor of correct shape and type here ###\n",
    "        \n",
    "        inputs = torch.tensor(inputs, dtype=torch.float)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.int64)\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            batch_indices = list(range(inputs.shape[0]))\n",
    "            random.shuffle(batch_indices)\n",
    "            batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for batch in batch_indices:\n",
    "                batch_importance = len(batch) / len(outputs)\n",
    "                batch_input = inputs[batch]\n",
    "                batch_output = outputs[batch]\n",
    "                ### make prediction and compute loss with loss function of your choice on this batch ###\n",
    "                batch_predictions = self.model(batch_input)\n",
    "                loss = nn.CrossEntropyLoss()(batch_predictions, batch_output)\n",
    "                if l2:\n",
    "                    ### Compute the loss with L2 regularization ###\n",
    "                    l2_lambda = 1e-5\n",
    "                    l2_norm = sum([p.pow(2.0).sum() for p in model.parameters()])\n",
    "                    loss = loss + l2_lambda * l2_norm\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ### Compute epoch_loss and epoch_acc\n",
    "                epoch_loss += loss.detach().item() * batch_importance\n",
    "                acc = torch.sum(pred == batch_output) / len(batch_predictions)\n",
    "                epoch_acc += acc.detach().item() * batch_importance\n",
    "            val_loss, val_acc = self.evaluate(val_inputs, val_outputs, print_acc=False)\n",
    "            if n_epoch % 10 ==0 and not silent: \n",
    "                print(\"Epoch %d/%d - Loss: %.3f - Acc: %.3f\" % (n_epoch + 1, self.epoch, epoch_loss, epoch_acc))\n",
    "                print(\"              Val_loss: %.3f - Val_acc: %.3f\" % (val_loss, val_acc))\n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "\n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"losses\": losses, \"accuracies\": accuracies, \"val_losses\": val_losses, \"val_accuracies\": val_accuracies}\n",
    "        \n",
    "    def evaluate(self, inputs, outputs, print_acc=True):\n",
    "        \"\"\" evaluate model on provided input and output\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        print_acc: bool\n",
    "        \n",
    "        @return\n",
    "        losses: float\n",
    "        acc: float\n",
    "        \"\"\"\n",
    "        inputs = self.input_transform(inputs)\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.int64)\n",
    "        self.model.eval() # change to evaluation mode so that it won't calculate gradient\n",
    "        batch_indices = list(range(inputs.shape[0]))\n",
    "        batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "        acc = 0\n",
    "        losses = 0\n",
    "        for batch in batch_indices:\n",
    "            batch_importance = len(batch) / len(outputs)\n",
    "            batch_input = inputs[batch]\n",
    "            batch_output = outputs[batch]\n",
    "            with torch.no_grad():\n",
    "                ### Compute prediction and loss###\n",
    "                batch_predictions = self.model(batch_input)\n",
    "                loss = nn.CrossEntropyLoss()(batch_predictions, batch_output)\n",
    "            pred = torch.argmax(batch_predictions, axis=-1)  \n",
    "            batch_acc = torch.sum(pred == batch_output) / len(batch_predictions)\n",
    "            losses += loss.detach().item() * batch_importance\n",
    "            acc += batch_acc.detach().item() * batch_importance\n",
    "        if print_acc:\n",
    "            print(\"Accuracy: %.3f\" % acc)\n",
    "        return losses, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0318ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1024, 3), #(_ , some number that's power of 2) #gradient vanishing \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3, 10),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c846e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, val_inputs, outputs, val_outputs = train_test_split(inputs, outputs, test_size=validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1512fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf2384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2e974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5dafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973c0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
