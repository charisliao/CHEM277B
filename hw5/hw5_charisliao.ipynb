{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf61c7a",
   "metadata": {},
   "source": [
    "# MSSE 277B: Machine Learning Algorithms \n",
    "\n",
    "## Homework assignment #5: Statistical Models \n",
    "\n",
    "### Student Name: Charis Liao \n",
    "\n",
    "#### Assigned Feb. 23 and Due Mar. 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81a693",
   "metadata": {},
   "source": [
    "# 1. Baye's Theorem\n",
    "\n",
    "A chemical test is designed to indirectly determine whether the individual has a\n",
    "genetic marker predisposing him/her to having kidney disease. The chemical test has the following\n",
    "characteristics: the probability that a randomly chosen person who has the marker (M) will test positive (i.e.\n",
    "\"marker present\") is: P[+|M] = 0.95; the probability that a randomly chosen person who does not have the\n",
    "marker will test negative is: P[-|not M] = 0.95; the proportion of people who have the marker is: P[M] =\n",
    "0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e169a94",
   "metadata": {},
   "source": [
    "**(a) Define the following quantities: P[-|M]; P[+|not M]; P[not M].**    \n",
    "\n",
    "$P[-|M] = 0.05$: The probability that a randomly chosen person who has the marker (M) will test negative.  \n",
    "\n",
    "$P[+|not M] = 0.05$: The probability that a randomly chosen person who does not have the marker (M) will test positive. \n",
    "\n",
    "$P[not M] = 0.99$: The proportion of people who do not have the marker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d9685",
   "metadata": {},
   "source": [
    "**(b) You have had a chemical test and have tested positive; should you be alarmed? To answer this,\n",
    "find what is the chance that a randomly selected person who tests positive for the marker actually has the\n",
    "marker by using Baye’s Theorem. What feature of the given data accounts for the result?**     \n",
    "\n",
    "$$ P[M|+] = \\frac{P[+|M]P[M]}{P[+|M]P[M] + P[+|not M]P[not M]} =$$   \n",
    "$$\\frac{0.95 \\times 0.01}{(0.95 \\times 0.01) + (0.05 \\times 0.99)} = 0.161 $$ \n",
    "\n",
    "No, you should not be alarmed because the probability of you actually having the marker given you've received a positive test is 0.161 which is relatively low. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f645e40",
   "metadata": {},
   "source": [
    "**(c) Suppose that frequency of marker was higher by a factor of 10, i.e. P[M] = 0.10. What is the\n",
    "chance that a randomly selected individual from this group who test positive actually has the marker?**    \n",
    "\n",
    "$$ P[M|+] = \\frac{P[+|M]P[M]}{P[+|M]P[M] + P[+|not M]P[not M]} =$$   \n",
    "$$\\frac{0.95 \\times 0.01}{(0.95 \\times 0.01) + (0.05 \\times 0.99)} = 0.6786 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de57cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6785714285714285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.95*0.1 / (0.95*0.1 + 0.05*0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d86d7e",
   "metadata": {},
   "source": [
    "The chance that a randomly selected individual from this group who test positive actually has the marker is 0.68 or 68%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c87475",
   "metadata": {},
   "source": [
    "# 2. Gaussian Naïve Bayes     \n",
    "In a previous homework we clustered 178 wines into 3 cultivars by solving\n",
    "the minimization of a cost function simulated annealing. We will do this again, but this time solving it with\n",
    "Naïve Bayes. To do this we will classify the wines by assigning them to the cultivar with the largest\n",
    "P(cultivar | X), and to find this we must first define a labelled data set of P(wine attribute x | cultivar)\n",
    "pairings to learn the relationship where x is one of the attributes, and do this for all attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffaf796",
   "metadata": {},
   "source": [
    "**(a) How should we represent P(wine attribute x | cultivar) ? Fill in the code for gaussian() function and\n",
    "give a reason that you choose this functional form. Given a wine that belongs to cultivar 1, what is the\n",
    "chance of it having an Alcohol % of 13 according to the probability distribution function?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5be599e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcbc5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self):\n",
    "        self.type_indices={}    # store the indices of wines that belong to each cultivar as a boolean array of length 178\n",
    "        self.type_stats={}      # store the mean and std of each cultivar\n",
    "        self.ndata = 0\n",
    "        self.trained=False\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian(x,mean,std):\n",
    "        exponent = -((x - mean)**2 / (2 * std**2))\n",
    "        return (1 / (np.sqrt(2 * np.pi) * std)) * np.exp(exponent)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(x_values):\n",
    "        # Returns a list with length of input features. Each element is a tuple, with the input feature's average and standard deviation\n",
    "        n_feats=x_values.shape[1]\n",
    "        return [(np.average(x_values[:,n]),np.std(x_values[:,n])) for n in range(n_feats)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_prob(x_input,stats):\n",
    "        \"\"\"Calculate the probability that the input features belong to a specific class(P(X|C)), defined by the statistics of features in that class\n",
    "        x_input: np.array shape(nfeatures)\n",
    "        stats: list of tuple [(mean1,std1),(means2,std2),...]\n",
    "        \"\"\" \n",
    "        init_prob = 1 \n",
    "        for i in range(len(x_input)):\n",
    "            mean, std = stats[i]\n",
    "            init_prob *= NaiveBayesClassifier.gaussian(x_input[i], mean, std)\n",
    "        return init_prob\n",
    "    \n",
    "    def fit(self,xs,ys):\n",
    "        # Train the classifier by calculating the statistics of different features in each class\n",
    "        self.ndata = len(ys)\n",
    "        for y in set(ys):\n",
    "            type_filter= (ys==y)\n",
    "            self.type_indices[y]=type_filter\n",
    "            self.type_stats[y]=self.calculate_statistics(xs[type_filter])\n",
    "        self.trained=True\n",
    "            \n",
    "    def predict(self,xs):\n",
    "        # Do the prediction by outputing the class that has highest probability\n",
    "        if len(xs.shape)>1:\n",
    "            print(\"Only accepts one sample at a time!\")\n",
    "        if self.trained:\n",
    "            guess=None\n",
    "            max_prob=0\n",
    "            # P(C|X) = P(X|C)*P(C) / sum_i(P(X|C_i)*P(C_i)) (deniminator for normalization only, can be ignored)\n",
    "            for y_type in self.type_stats:\n",
    "                prior = sum(self.type_indices[y_type]) / self.ndata\n",
    "                prob= self.calculate_prob(xs, self.type_stats[y_type]) * prior\n",
    "                if prob>max_prob:\n",
    "                    max_prob=prob\n",
    "                    guess=y_type\n",
    "            return guess\n",
    "        else:\n",
    "            print(\"Please train the classifier first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86ad6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model,xs,ys):\n",
    "    y_pred=np.zeros_like(ys)\n",
    "    for idx,x in enumerate(xs):\n",
    "        y_pred[idx]=model.predict(x)\n",
    "    return np.sum(ys==y_pred)/len(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d0ee3",
   "metadata": {},
   "source": [
    "The reason why gaussian is chosen is that it is the mathematical representation of the normal distribution, which is one of the most widely used probability distributions in statistics and data analysis. It has a bell-shaped curve and has mean and standard deviation as parameters, which make it useful for modeling many real-world phenomena. The Gaussian function is also mathematically tractable and has a well-defined shape, making it easy to work with in calculations and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92cf34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wine dataset \n",
    "df=pd.read_csv('./wines.csv') \n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3aa77336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chance of a wine from cultivar 1 having Alcohol % of 13 is: 0.23236757865410362\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the classifier\n",
    "clf = NaiveBayesClassifier() \n",
    "# train the classifier on the dataset\n",
    "clf.fit(df.iloc[:,0:-2].values, df.iloc[:,-1].values)\n",
    "\n",
    "# calculate the probability that a wine from cultivar 1 has Alcohol % = 13\n",
    "stats = clf.type_stats[1]  # get the statistics for cultivar 1\n",
    "\n",
    "prob = clf.gaussian(13, stats[0][0], stats[0][1])  # calculate the probability using the Gaussian function\n",
    "\n",
    "print(\"The chance of a wine from cultivar 1 having Alcohol % of 13 is:\", prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936b22e",
   "metadata": {},
   "source": [
    "**(b) Using your normalized chemical descriptor data from HW#2, divide your data into 3-fold training\n",
    "and testing groups, i.e. using 2/3** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0307dd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.514341</td>\n",
       "      <td>-0.560668</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>-1.166303</td>\n",
       "      <td>1.908522</td>\n",
       "      <td>0.806722</td>\n",
       "      <td>1.031908</td>\n",
       "      <td>-0.657708</td>\n",
       "      <td>1.221438</td>\n",
       "      <td>0.251009</td>\n",
       "      <td>0.361158</td>\n",
       "      <td>1.842721</td>\n",
       "      <td>1.010159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.294868</td>\n",
       "      <td>0.227053</td>\n",
       "      <td>1.835226</td>\n",
       "      <td>0.450674</td>\n",
       "      <td>1.278379</td>\n",
       "      <td>0.806722</td>\n",
       "      <td>0.661485</td>\n",
       "      <td>0.226158</td>\n",
       "      <td>0.400275</td>\n",
       "      <td>-0.318377</td>\n",
       "      <td>0.361158</td>\n",
       "      <td>0.448336</td>\n",
       "      <td>-0.037767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.253415</td>\n",
       "      <td>-0.623328</td>\n",
       "      <td>-0.716315</td>\n",
       "      <td>-1.645408</td>\n",
       "      <td>-0.191954</td>\n",
       "      <td>0.806722</td>\n",
       "      <td>0.951817</td>\n",
       "      <td>-0.577356</td>\n",
       "      <td>0.679820</td>\n",
       "      <td>0.061213</td>\n",
       "      <td>0.536158</td>\n",
       "      <td>0.335659</td>\n",
       "      <td>0.946649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.378844</td>\n",
       "      <td>-0.766550</td>\n",
       "      <td>-0.169557</td>\n",
       "      <td>-0.806975</td>\n",
       "      <td>-0.331985</td>\n",
       "      <td>-0.151973</td>\n",
       "      <td>0.401188</td>\n",
       "      <td>-0.818411</td>\n",
       "      <td>-0.036514</td>\n",
       "      <td>-0.025057</td>\n",
       "      <td>0.929908</td>\n",
       "      <td>0.293405</td>\n",
       "      <td>1.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.923081</td>\n",
       "      <td>-0.542765</td>\n",
       "      <td>0.158499</td>\n",
       "      <td>-1.046527</td>\n",
       "      <td>-0.752080</td>\n",
       "      <td>0.487157</td>\n",
       "      <td>0.731565</td>\n",
       "      <td>-0.577356</td>\n",
       "      <td>0.382804</td>\n",
       "      <td>0.233755</td>\n",
       "      <td>0.842408</td>\n",
       "      <td>0.406082</td>\n",
       "      <td>1.819921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.491955</td>\n",
       "      <td>2.026281</td>\n",
       "      <td>1.798775</td>\n",
       "      <td>1.648436</td>\n",
       "      <td>0.858284</td>\n",
       "      <td>-0.503494</td>\n",
       "      <td>-1.070491</td>\n",
       "      <td>-0.738059</td>\n",
       "      <td>-0.840205</td>\n",
       "      <td>1.484679</td>\n",
       "      <td>-1.257591</td>\n",
       "      <td>-0.974218</td>\n",
       "      <td>-0.371199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.331822</td>\n",
       "      <td>1.739837</td>\n",
       "      <td>-0.388260</td>\n",
       "      <td>0.151234</td>\n",
       "      <td>1.418411</td>\n",
       "      <td>-1.126646</td>\n",
       "      <td>-1.340800</td>\n",
       "      <td>0.547563</td>\n",
       "      <td>-0.420888</td>\n",
       "      <td>2.217979</td>\n",
       "      <td>-1.607590</td>\n",
       "      <td>-1.481267</td>\n",
       "      <td>0.279786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.208643</td>\n",
       "      <td>0.227053</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.151234</td>\n",
       "      <td>1.418411</td>\n",
       "      <td>-1.030776</td>\n",
       "      <td>-1.350811</td>\n",
       "      <td>1.351077</td>\n",
       "      <td>-0.228701</td>\n",
       "      <td>1.829761</td>\n",
       "      <td>-1.563840</td>\n",
       "      <td>-1.396759</td>\n",
       "      <td>0.295664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.391162</td>\n",
       "      <td>1.578712</td>\n",
       "      <td>1.361368</td>\n",
       "      <td>1.498716</td>\n",
       "      <td>-0.261969</td>\n",
       "      <td>-0.391646</td>\n",
       "      <td>-1.270720</td>\n",
       "      <td>1.592131</td>\n",
       "      <td>-0.420888</td>\n",
       "      <td>1.786626</td>\n",
       "      <td>-1.520090</td>\n",
       "      <td>-1.424928</td>\n",
       "      <td>-0.593486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.924604</td>\n",
       "      <td>-0.542765</td>\n",
       "      <td>-0.898568</td>\n",
       "      <td>-0.148206</td>\n",
       "      <td>-1.382223</td>\n",
       "      <td>-1.030776</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.065455</td>\n",
       "      <td>0.068316</td>\n",
       "      <td>-0.715222</td>\n",
       "      <td>0.186159</td>\n",
       "      <td>0.786369</td>\n",
       "      <td>-0.752263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol %  Malic Acid       Ash  Alkalinity        Mg   Phenols  \\\n",
       "0     1.514341   -0.560668  0.231400   -1.166303  1.908522  0.806722   \n",
       "1     0.294868    0.227053  1.835226    0.450674  1.278379  0.806722   \n",
       "2     2.253415   -0.623328 -0.716315   -1.645408 -0.191954  0.806722   \n",
       "3     1.378844   -0.766550 -0.169557   -0.806975 -0.331985 -0.151973   \n",
       "4     0.923081   -0.542765  0.158499   -1.046527 -0.752080  0.487157   \n",
       "..         ...         ...       ...         ...       ...       ...   \n",
       "173   0.491955    2.026281  1.798775    1.648436  0.858284 -0.503494   \n",
       "174   0.331822    1.739837 -0.388260    0.151234  1.418411 -1.126646   \n",
       "175   0.208643    0.227053  0.012696    0.151234  1.418411 -1.030776   \n",
       "176   1.391162    1.578712  1.361368    1.498716 -0.261969 -0.391646   \n",
       "177  -0.924604   -0.542765 -0.898568   -0.148206 -1.382223 -1.030776   \n",
       "\n",
       "     Flavanoids  Phenols.1  Proantho-cyanins  Color intensity       Hue  \\\n",
       "0      1.031908  -0.657708          1.221438         0.251009  0.361158   \n",
       "1      0.661485   0.226158          0.400275        -0.318377  0.361158   \n",
       "2      0.951817  -0.577356          0.679820         0.061213  0.536158   \n",
       "3      0.401188  -0.818411         -0.036514        -0.025057  0.929908   \n",
       "4      0.731565  -0.577356          0.382804         0.233755  0.842408   \n",
       "..          ...        ...               ...              ...       ...   \n",
       "173   -1.070491  -0.738059         -0.840205         1.484679 -1.257591   \n",
       "174   -1.340800   0.547563         -0.420888         2.217979 -1.607590   \n",
       "175   -1.350811   1.351077         -0.228701         1.829761 -1.563840   \n",
       "176   -1.270720   1.592131         -0.420888         1.786626 -1.520090   \n",
       "177    0.000731   0.065455          0.068316        -0.715222  0.186159   \n",
       "\n",
       "     OD280 315   Proline  \n",
       "0     1.842721  1.010159  \n",
       "1     0.448336 -0.037767  \n",
       "2     0.335659  0.946649  \n",
       "3     0.293405  1.692900  \n",
       "4     0.406082  1.819921  \n",
       "..         ...       ...  \n",
       "173  -0.974218 -0.371199  \n",
       "174  -1.481267  0.279786  \n",
       "175  -1.396759  0.295664  \n",
       "176  -1.424928 -0.593486  \n",
       "177   0.786369 -0.752263  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized chemical descriptor\n",
    "df_norm = (df.iloc[:,0:-2] - df.iloc[:,0:-2].mean()) / df.iloc[:,0:-2].std()\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3335ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9833333333333333\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.9661016949152542\n"
     ]
    }
   ],
   "source": [
    "# divide the data into 3-fold training and testing groups\n",
    "# use 2/3 training and 1/3 testing for the three divisions\n",
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "xs = df_norm.iloc[:,:].values \n",
    "ys = df.iloc[:,-1].values\n",
    "for train_index, test_index in kf.split(xs):\n",
    "    x_train, x_test = xs[train_index], xs[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "\n",
    "    # train the classifier\n",
    "    gnb = NaiveBayesClassifier()\n",
    "    gnb.fit(x_train,y_train)\n",
    "\n",
    "    print(f'Accuracy: {calculate_accuracy(gnb,x_test,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfd51c",
   "metadata": {},
   "source": [
    "This performance is better than the previous one with an average accuracy above 95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907eb8be",
   "metadata": {},
   "source": [
    "# 3. Softmax and Cross Entropy Loss.\n",
    "\n",
    "Work on the same wine dataset. Now we use another approach\n",
    "to do the classification. Implement a neural network model using PyTorch with no hidden layer (This is\n",
    "equivalent to a linear regression plus activation function). Use softmax activation function in the last layer\n",
    "and use cross entropy loss as your loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf67961",
   "metadata": {},
   "source": [
    "**(a) Pass the data through the network once without backpropagation and print out the output. Observe\n",
    "the difference between with and without the softmax activation layer. What does softmax do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c3004bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2842e+00,  4.4496e-01, -6.1235e-01],\n",
      "        [ 3.9544e-01,  4.9609e-01, -3.8322e-01],\n",
      "        [ 1.1597e+00,  4.4352e-01, -3.3465e-01],\n",
      "        [ 3.2252e-01,  2.0005e-01,  1.1345e-01],\n",
      "        [ 6.5048e-01,  3.1566e-01,  2.7726e-01],\n",
      "        [ 6.0775e-01,  3.1100e-01, -3.6272e-01],\n",
      "        [ 1.4105e+00,  3.9178e-01, -6.0870e-01],\n",
      "        [ 5.3006e-01,  2.5589e-01,  2.5212e-01],\n",
      "        [ 6.4879e-01,  7.7252e-01,  8.0664e-02],\n",
      "        [ 7.5103e-02, -2.7768e-01,  4.7783e-01],\n",
      "        [ 9.2738e-01,  3.9420e-01,  8.6092e-02],\n",
      "        [ 5.6992e-02,  2.5956e-01, -3.9037e-02],\n",
      "        [ 6.9960e-01,  1.8454e-01, -6.8461e-01],\n",
      "        [ 1.8242e-01, -7.3868e-02,  1.1511e-01],\n",
      "        [ 1.7620e+00,  3.8266e-01, -5.9698e-01],\n",
      "        [ 1.1067e+00, -7.9319e-02,  1.6791e-01],\n",
      "        [ 8.2152e-01,  4.3073e-01,  3.1767e-02],\n",
      "        [ 8.9470e-01,  6.4870e-01,  5.7756e-02],\n",
      "        [ 1.7622e+00,  2.6806e-01, -2.1963e-01],\n",
      "        [ 1.6217e+00, -9.2042e-02, -2.9555e-01],\n",
      "        [-1.0329e+00, -5.3258e-01,  5.1092e-01],\n",
      "        [ 1.2892e+00,  1.3135e-01,  5.3768e-01],\n",
      "        [ 1.5581e+00,  2.7848e-01,  6.4785e-01],\n",
      "        [-1.0756e+00, -9.0570e-01,  3.9814e-01],\n",
      "        [-8.4489e-01, -4.3631e-01,  1.4767e-01],\n",
      "        [-6.3074e-01,  2.7322e-01,  1.1638e+00],\n",
      "        [ 1.0228e+00, -3.7839e-01, -1.6055e-01],\n",
      "        [ 2.5433e-01,  8.2543e-02,  1.8103e-01],\n",
      "        [-5.7848e-01, -2.5644e-04,  9.8561e-01],\n",
      "        [-3.4528e-01,  2.7442e-01,  1.3957e+00],\n",
      "        [-1.2442e-01, -9.3906e-01, -2.8418e-01],\n",
      "        [-6.4222e-01, -9.8526e-01, -6.8511e-01],\n",
      "        [ 2.1009e-01,  8.8901e-01,  7.7218e-01],\n",
      "        [-2.5889e-01, -2.5533e-01,  9.7076e-01],\n",
      "        [-1.3828e-01, -5.1225e-01,  9.3037e-01],\n",
      "        [ 6.3361e-01, -4.7893e-01,  7.2010e-01],\n",
      "        [ 4.1257e-02, -3.2805e-01,  7.1981e-01],\n",
      "        [ 2.8963e-01,  2.7026e-01,  6.5880e-01],\n",
      "        [-7.2643e-01,  4.8096e-01,  1.7526e+00],\n",
      "        [ 6.6509e-01, -7.9541e-02,  5.1956e-01],\n",
      "        [ 1.1197e-01,  9.4111e-01,  2.8417e-01],\n",
      "        [ 1.0279e-01,  1.7414e-01,  6.6526e-01],\n",
      "        [-6.5089e-01,  2.9148e-01,  7.3506e-01],\n",
      "        [-7.0174e-01, -7.1748e-01, -7.9839e-02],\n",
      "        [-7.0468e-01, -7.2199e-01, -3.3176e-01],\n",
      "        [-9.7541e-01,  1.1967e-01,  4.1501e-01],\n",
      "        [-8.8446e-01,  3.5637e-02,  4.7893e-01],\n",
      "        [-1.0972e+00,  3.2236e-01, -2.6910e-02],\n",
      "        [-1.1431e+00,  1.3677e-01, -2.5014e-01],\n",
      "        [-3.5306e-01, -4.7422e-01, -4.2023e-01],\n",
      "        [ 2.0172e-01, -6.8194e-01, -5.9703e-01],\n",
      "        [-1.4677e-02,  2.3042e-01, -1.6127e-01],\n",
      "        [-9.0308e-01,  7.3999e-02, -1.3888e-01],\n",
      "        [-4.7637e-01,  6.0871e-02, -3.4093e-02],\n",
      "        [-3.4141e-01, -2.3586e-01,  3.5145e-01],\n",
      "        [ 1.4857e-01, -1.0510e-02, -2.1829e-01],\n",
      "        [-9.7861e-01,  3.8420e-01,  4.0758e-02],\n",
      "        [-7.3946e-01, -1.2384e-01,  1.1268e-01],\n",
      "        [ 9.1057e-01, -3.7245e-02, -1.5081e-01],\n",
      "        [ 1.7871e+00,  2.6044e-01, -1.8347e-01],\n",
      "        [ 1.1033e+00,  1.6087e-01, -1.5020e-01],\n",
      "        [ 1.5586e+00,  3.6344e-01,  6.5991e-03],\n",
      "        [ 1.4744e+00,  8.8358e-01, -2.7057e-01],\n",
      "        [ 6.4672e-01,  3.0392e-01, -3.7608e-02],\n",
      "        [ 4.2246e-01,  7.3618e-01, -1.5858e-01],\n",
      "        [ 2.1508e-02,  6.9291e-01, -1.9008e-01],\n",
      "        [ 7.1080e-01,  9.1116e-01, -1.6602e-01],\n",
      "        [ 6.6562e-01,  4.3784e-01,  3.7542e-01],\n",
      "        [ 2.0939e-01,  4.2325e-01,  1.4702e-01],\n",
      "        [ 2.2498e-01,  1.4637e-01, -2.0919e-01],\n",
      "        [ 2.3652e-01, -1.7646e-01,  2.7243e-01],\n",
      "        [ 1.3320e+00,  6.0800e-01, -1.3635e+00],\n",
      "        [ 4.5600e-01,  3.9908e-01, -1.8870e-01],\n",
      "        [ 3.6106e-01,  6.4187e-01, -2.5383e-01],\n",
      "        [ 1.5842e+00,  2.0645e-01, -8.2759e-02],\n",
      "        [ 1.4803e+00, -3.2441e-02,  2.3439e-01],\n",
      "        [ 5.4896e-01,  2.3679e-01, -6.0804e-02],\n",
      "        [ 9.5714e-01, -1.3655e-01, -3.9735e-01],\n",
      "        [-4.2550e-01, -1.3549e+00,  4.2705e-01],\n",
      "        [-1.7346e-01, -1.2581e-02,  3.3884e-01],\n",
      "        [-6.4246e-01,  1.0998e-01,  1.8427e-01],\n",
      "        [ 3.2970e-01,  1.6597e-01,  1.0138e+00],\n",
      "        [ 7.8723e-01,  1.1865e+00,  2.0441e-01],\n",
      "        [-5.7148e-02,  2.9744e-01,  3.6569e-01],\n",
      "        [ 4.3514e-01, -1.8553e-01,  3.8500e-01],\n",
      "        [ 1.3303e-02, -1.0945e+00, -1.9360e-01],\n",
      "        [ 2.2376e-01,  1.6997e-01,  8.5274e-01],\n",
      "        [ 3.7596e-01,  3.0945e-01,  5.4051e-01],\n",
      "        [-8.2393e-01,  4.7911e-01,  1.2346e+00],\n",
      "        [-5.6077e-01,  1.9051e-02,  6.5522e-01],\n",
      "        [-6.4238e-01,  4.3492e-02,  7.0993e-01],\n",
      "        [ 4.1731e-01,  5.0140e-01,  2.7404e-01],\n",
      "        [-3.5315e-01, -3.1381e-01,  5.6001e-01],\n",
      "        [ 3.5935e-01,  1.7191e-01,  1.5725e-01],\n",
      "        [ 4.1078e-02,  1.7395e-01,  1.0127e+00],\n",
      "        [ 5.0037e-01, -2.1067e-01,  5.8702e-01],\n",
      "        [ 8.9376e-01,  1.1163e-02,  3.0275e-01],\n",
      "        [-8.5806e-01,  4.1157e-01,  2.7678e-01],\n",
      "        [-9.6722e-02, -2.3507e-01,  9.4615e-01],\n",
      "        [-3.9495e-01, -1.4531e-01,  1.4242e-01],\n",
      "        [-4.8320e-01,  6.2406e-02,  8.0374e-01],\n",
      "        [ 1.0144e+00,  8.7530e-02,  3.6440e-01],\n",
      "        [-6.6069e-01,  3.6835e-01,  1.0106e+00],\n",
      "        [-5.4573e-01, -6.2640e-01, -5.2487e-01],\n",
      "        [-1.7094e+00, -2.5468e-01,  3.8429e-01],\n",
      "        [-8.2370e-01,  3.5608e-02,  3.5149e-02],\n",
      "        [-5.4384e-01, -1.7992e-01,  1.5255e-01],\n",
      "        [-9.6091e-01,  5.3985e-01,  1.2385e-01],\n",
      "        [-1.1185e+00, -6.1456e-01,  2.1978e-01],\n",
      "        [-2.0677e-01, -2.5746e-01,  1.7023e-01],\n",
      "        [-1.1765e+00, -2.4897e-01,  1.0334e+00],\n",
      "        [-3.2618e-01, -3.0998e-01,  4.5370e-01],\n",
      "        [-1.0823e+00,  7.6693e-02,  2.5077e-01],\n",
      "        [-8.0450e-01, -5.2307e-01, -2.7541e-01],\n",
      "        [-1.1166e+00,  3.8553e-01,  3.6051e-01],\n",
      "        [-7.9553e-01, -6.5195e-01,  4.7264e-02],\n",
      "        [-1.9188e-01, -4.6946e-01,  2.4376e-01],\n",
      "        [ 9.6702e-01,  4.6347e-01,  1.2813e-01],\n",
      "        [ 9.4945e-01,  5.6304e-01, -1.6890e-01],\n",
      "        [ 3.8103e-01,  3.3644e-01, -3.3615e-01],\n",
      "        [ 7.0403e-01,  4.3279e-01,  3.5721e-01],\n",
      "        [ 1.6028e+00,  4.7104e-01, -9.3926e-02],\n",
      "        [ 5.9544e-01,  3.2240e-01, -7.4815e-02],\n",
      "        [ 1.0962e+00,  3.4838e-01,  2.4180e-01],\n",
      "        [ 1.0651e+00,  3.9620e-01, -9.0549e-01],\n",
      "        [ 8.2869e-01,  6.1846e-01,  2.7530e-02],\n",
      "        [ 9.1003e-01,  2.5999e-01, -1.3031e-02],\n",
      "        [ 6.8993e-01, -3.7007e-02,  4.0303e-01],\n",
      "        [ 8.2881e-01,  4.8405e-01,  2.2118e-01],\n",
      "        [ 1.4722e+00,  2.6070e-01, -2.9410e-01],\n",
      "        [ 9.1487e-02,  4.0011e-01,  4.2860e-01],\n",
      "        [ 1.0255e+00,  8.6673e-01, -1.4411e-01],\n",
      "        [ 1.1512e+00,  3.9667e-01,  2.4519e-01],\n",
      "        [ 9.6131e-01, -9.3444e-03, -2.6005e-02],\n",
      "        [ 1.4288e+00,  1.9567e-01, -4.5392e-01],\n",
      "        [ 8.4001e-01,  2.0822e-01, -2.7966e-02],\n",
      "        [-3.5284e-01, -6.1907e-01,  3.0411e-01],\n",
      "        [ 4.5711e-01, -2.0248e-02,  1.4748e-01],\n",
      "        [-4.2309e-01, -5.2355e-01, -3.8921e-02],\n",
      "        [-3.1870e-01, -1.1334e+00, -8.3360e-02],\n",
      "        [-9.8601e-02,  2.0686e-01,  5.7566e-01],\n",
      "        [ 6.7841e-01, -6.0660e-01,  6.3068e-01],\n",
      "        [-5.6110e-01, -5.4091e-01,  5.1345e-01],\n",
      "        [-2.1759e-01,  9.1284e-01,  6.0135e-01],\n",
      "        [-8.2802e-01,  5.1929e-01,  6.5727e-01],\n",
      "        [-8.4510e-01,  1.1045e-01,  9.1528e-01],\n",
      "        [-5.7747e-01,  8.5945e-02,  8.0993e-01],\n",
      "        [ 6.0596e-01, -1.8143e-01,  2.3740e-01],\n",
      "        [ 5.3075e-01,  8.1801e-03,  5.1886e-01],\n",
      "        [ 1.4050e+00, -8.6773e-02,  5.9027e-01],\n",
      "        [-5.0075e-01,  5.9967e-01,  7.9535e-01],\n",
      "        [-4.3323e-01,  1.1093e-01,  6.0182e-01],\n",
      "        [ 2.4013e-01,  6.1040e-02,  2.9451e-01],\n",
      "        [-1.8720e-01, -3.7023e-02,  3.2467e-01],\n",
      "        [ 1.1467e+00,  9.2968e-01,  1.9220e-01],\n",
      "        [-6.1364e-01,  8.7608e-01,  3.2598e-01],\n",
      "        [ 1.0003e+00,  7.1453e-01,  3.9936e-01],\n",
      "        [ 4.3188e-01,  2.1223e-01,  6.9053e-01],\n",
      "        [-3.2432e-01, -1.3476e+00, -8.5910e-01],\n",
      "        [-3.5610e-01, -7.3463e-01,  5.7664e-01],\n",
      "        [-7.8530e-01, -5.3485e-01,  5.7622e-01],\n",
      "        [-1.6818e+00,  4.4181e-01,  6.0837e-01],\n",
      "        [-6.3855e-01, -8.1052e-01,  3.8522e-02],\n",
      "        [-6.7266e-01,  4.3290e-02,  2.7118e-01],\n",
      "        [-1.2323e-01, -5.1843e-01, -9.5334e-01],\n",
      "        [ 3.1048e-01, -7.6470e-01, -5.6127e-01],\n",
      "        [-6.7286e-02, -2.4482e-01,  2.2744e-01],\n",
      "        [-3.8801e-01, -1.0561e+00,  4.0607e-01],\n",
      "        [-1.0620e+00,  1.2428e-01,  5.5559e-01],\n",
      "        [ 1.2284e+00,  3.1505e-01,  1.9261e-01],\n",
      "        [ 9.7349e-01, -2.7388e-02,  3.0772e-01],\n",
      "        [-3.2450e-01, -5.8986e-02,  1.6041e-01],\n",
      "        [-3.5117e-01, -8.7337e-02, -5.6746e-02],\n",
      "        [-3.7816e-01,  1.4054e-01, -4.8255e-01],\n",
      "        [-6.2612e-01, -5.2954e-01, -1.3351e-01],\n",
      "        [-4.3954e-01, -7.2467e-01, -6.0516e-02],\n",
      "        [-3.4762e-01,  5.0073e-01, -5.3531e-02],\n",
      "        [ 5.9107e-02,  1.8810e-01,  7.1582e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Convert the features and labels to PyTorch tensors\n",
    "features = torch.tensor(df_norm.iloc[:,:].values, dtype=torch.float32)\n",
    "labels = torch.tensor(df.iloc[:,-1].values, dtype=torch.int64)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(features.shape[1], len(np.unique(labels)))\n",
    "#     nn.identity()\n",
    ")\n",
    "\n",
    "# Pass the data through the network once without backpropagation\n",
    "outputs = model(features)\n",
    "\n",
    "# Print out the output\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e77e52b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3138, 0.1741, 0.5121],\n",
      "        [0.2444, 0.2509, 0.5047],\n",
      "        [0.3072, 0.2180, 0.4747],\n",
      "        [0.2992, 0.2659, 0.4349],\n",
      "        [0.3448, 0.2535, 0.4017],\n",
      "        [0.3274, 0.2352, 0.4374],\n",
      "        [0.3210, 0.2426, 0.4364],\n",
      "        [0.3049, 0.3037, 0.3914],\n",
      "        [0.2541, 0.3020, 0.4439],\n",
      "        [0.3640, 0.3240, 0.3120],\n",
      "        [0.3266, 0.2097, 0.4637],\n",
      "        [0.2105, 0.4878, 0.3017],\n",
      "        [0.2308, 0.2340, 0.5352],\n",
      "        [0.2837, 0.3086, 0.4076],\n",
      "        [0.4471, 0.1804, 0.3726],\n",
      "        [0.4497, 0.2753, 0.2750],\n",
      "        [0.3469, 0.2081, 0.4449],\n",
      "        [0.2858, 0.1857, 0.5285],\n",
      "        [0.4526, 0.2860, 0.2615],\n",
      "        [0.5115, 0.1987, 0.2897],\n",
      "        [0.1236, 0.6250, 0.2514],\n",
      "        [0.2782, 0.4990, 0.2229],\n",
      "        [0.3666, 0.3879, 0.2455],\n",
      "        [0.2731, 0.2486, 0.4783],\n",
      "        [0.2596, 0.3437, 0.3967],\n",
      "        [0.1454, 0.5499, 0.3047],\n",
      "        [0.2851, 0.1395, 0.5754],\n",
      "        [0.1763, 0.4127, 0.4110],\n",
      "        [0.2585, 0.4327, 0.3088],\n",
      "        [0.2090, 0.5467, 0.2443],\n",
      "        [0.1688, 0.1935, 0.6378],\n",
      "        [0.1848, 0.1203, 0.6949],\n",
      "        [0.1590, 0.3859, 0.4550],\n",
      "        [0.2628, 0.5337, 0.2035],\n",
      "        [0.2745, 0.4858, 0.2397],\n",
      "        [0.4713, 0.1418, 0.3869],\n",
      "        [0.1998, 0.5239, 0.2763],\n",
      "        [0.2502, 0.5390, 0.2108],\n",
      "        [0.0707, 0.7579, 0.1713],\n",
      "        [0.4204, 0.3181, 0.2615],\n",
      "        [0.4612, 0.1670, 0.3718],\n",
      "        [0.2657, 0.4365, 0.2979],\n",
      "        [0.3843, 0.2654, 0.3503],\n",
      "        [0.4464, 0.1808, 0.3728],\n",
      "        [0.4174, 0.1563, 0.4263],\n",
      "        [0.3461, 0.2220, 0.4318],\n",
      "        [0.2885, 0.3339, 0.3776],\n",
      "        [0.2337, 0.2374, 0.5289],\n",
      "        [0.3705, 0.0849, 0.5446],\n",
      "        [0.6034, 0.0876, 0.3089],\n",
      "        [0.4081, 0.1208, 0.4710],\n",
      "        [0.4795, 0.0658, 0.4547],\n",
      "        [0.2577, 0.2005, 0.5417],\n",
      "        [0.4759, 0.1042, 0.4199],\n",
      "        [0.5968, 0.1079, 0.2953],\n",
      "        [0.4745, 0.0853, 0.4401],\n",
      "        [0.4735, 0.0836, 0.4429],\n",
      "        [0.4375, 0.1136, 0.4489],\n",
      "        [0.3597, 0.2858, 0.3545],\n",
      "        [0.5630, 0.1654, 0.2716],\n",
      "        [0.4105, 0.2503, 0.3392],\n",
      "        [0.4714, 0.2356, 0.2931],\n",
      "        [0.2190, 0.1835, 0.5974],\n",
      "        [0.3245, 0.3882, 0.2873],\n",
      "        [0.3075, 0.1365, 0.5559],\n",
      "        [0.1736, 0.3130, 0.5134],\n",
      "        [0.2084, 0.3303, 0.4613],\n",
      "        [0.3147, 0.3066, 0.3787],\n",
      "        [0.2116, 0.3343, 0.4541],\n",
      "        [0.2645, 0.2643, 0.4712],\n",
      "        [0.2789, 0.4127, 0.3084],\n",
      "        [0.3063, 0.0775, 0.6162],\n",
      "        [0.4244, 0.1853, 0.3903],\n",
      "        [0.4548, 0.2043, 0.3409],\n",
      "        [0.4860, 0.2167, 0.2972],\n",
      "        [0.4532, 0.1382, 0.4086],\n",
      "        [0.3257, 0.2821, 0.3922],\n",
      "        [0.4389, 0.2227, 0.3384],\n",
      "        [0.2366, 0.4693, 0.2942],\n",
      "        [0.2059, 0.5377, 0.2565],\n",
      "        [0.1047, 0.5160, 0.3793],\n",
      "        [0.3114, 0.5103, 0.1783],\n",
      "        [0.1668, 0.4271, 0.4062],\n",
      "        [0.2932, 0.4008, 0.3059],\n",
      "        [0.2731, 0.4093, 0.3176],\n",
      "        [0.2047, 0.1843, 0.6110],\n",
      "        [0.1495, 0.6332, 0.2173],\n",
      "        [0.2953, 0.3520, 0.3527],\n",
      "        [0.1461, 0.5832, 0.2707],\n",
      "        [0.1862, 0.3403, 0.4735],\n",
      "        [0.2145, 0.3573, 0.4282],\n",
      "        [0.2290, 0.2748, 0.4962],\n",
      "        [0.2552, 0.3531, 0.3917],\n",
      "        [0.3392, 0.3190, 0.3418],\n",
      "        [0.3011, 0.4502, 0.2487],\n",
      "        [0.3039, 0.3576, 0.3385],\n",
      "        [0.2835, 0.2566, 0.4599],\n",
      "        [0.1666, 0.4239, 0.4096],\n",
      "        [0.2925, 0.4181, 0.2894],\n",
      "        [0.3839, 0.2240, 0.3921],\n",
      "        [0.2977, 0.2805, 0.4218],\n",
      "        [0.4146, 0.3702, 0.2152],\n",
      "        [0.2190, 0.5140, 0.2670],\n",
      "        [0.3863, 0.1393, 0.4744],\n",
      "        [0.3298, 0.1720, 0.4982],\n",
      "        [0.2761, 0.1946, 0.5293],\n",
      "        [0.4133, 0.1663, 0.4204],\n",
      "        [0.2911, 0.1757, 0.5332],\n",
      "        [0.3959, 0.1410, 0.4631],\n",
      "        [0.5473, 0.1097, 0.3431],\n",
      "        [0.4791, 0.2536, 0.2673],\n",
      "        [0.5721, 0.1574, 0.2705],\n",
      "        [0.2873, 0.2452, 0.4675],\n",
      "        [0.3710, 0.1252, 0.5038],\n",
      "        [0.3811, 0.1466, 0.4723],\n",
      "        [0.4165, 0.1569, 0.4266],\n",
      "        [0.5953, 0.1385, 0.2662],\n",
      "        [0.3588, 0.1509, 0.4904],\n",
      "        [0.3145, 0.1492, 0.5364],\n",
      "        [0.3201, 0.2640, 0.4159],\n",
      "        [0.3677, 0.2638, 0.3685],\n",
      "        [0.3663, 0.1392, 0.4945],\n",
      "        [0.3270, 0.3094, 0.3636],\n",
      "        [0.4522, 0.3320, 0.2158],\n",
      "        [0.3554, 0.1379, 0.5067],\n",
      "        [0.3006, 0.2981, 0.4013],\n",
      "        [0.3845, 0.2611, 0.3544],\n",
      "        [0.4706, 0.2745, 0.2549],\n",
      "        [0.3689, 0.3085, 0.3226],\n",
      "        [0.3911, 0.2340, 0.3749],\n",
      "        [0.4742, 0.2451, 0.2807],\n",
      "        [0.3771, 0.1830, 0.4399],\n",
      "        [0.4930, 0.2725, 0.2345],\n",
      "        [0.4104, 0.1822, 0.4073],\n",
      "        [0.4102, 0.2066, 0.3832],\n",
      "        [0.3824, 0.2719, 0.3457],\n",
      "        [0.2787, 0.4862, 0.2351],\n",
      "        [0.2272, 0.3139, 0.4589],\n",
      "        [0.1718, 0.5271, 0.3010],\n",
      "        [0.1859, 0.2239, 0.5902],\n",
      "        [0.1966, 0.6034, 0.2001],\n",
      "        [0.3540, 0.3911, 0.2549],\n",
      "        [0.2339, 0.4365, 0.3295],\n",
      "        [0.2210, 0.4238, 0.3552],\n",
      "        [0.3388, 0.2585, 0.4026],\n",
      "        [0.1433, 0.5238, 0.3329],\n",
      "        [0.2173, 0.3505, 0.4322],\n",
      "        [0.2717, 0.4147, 0.3136],\n",
      "        [0.2242, 0.4496, 0.3261],\n",
      "        [0.3550, 0.4861, 0.1588],\n",
      "        [0.2492, 0.4531, 0.2977],\n",
      "        [0.2768, 0.3300, 0.3931],\n",
      "        [0.3062, 0.4026, 0.2912],\n",
      "        [0.2113, 0.4511, 0.3376],\n",
      "        [0.3316, 0.5520, 0.1164],\n",
      "        [0.2638, 0.2849, 0.4513],\n",
      "        [0.4396, 0.1241, 0.4363],\n",
      "        [0.3343, 0.4688, 0.1969],\n",
      "        [0.3033, 0.1718, 0.5249],\n",
      "        [0.3278, 0.2683, 0.4039],\n",
      "        [0.4278, 0.2529, 0.3193],\n",
      "        [0.3325, 0.2111, 0.4564],\n",
      "        [0.5920, 0.0862, 0.3218],\n",
      "        [0.5607, 0.1048, 0.3345],\n",
      "        [0.4985, 0.0807, 0.4208],\n",
      "        [0.6595, 0.0646, 0.2760],\n",
      "        [0.5576, 0.0797, 0.3627],\n",
      "        [0.4867, 0.1864, 0.3269],\n",
      "        [0.5186, 0.1028, 0.3786],\n",
      "        [0.5724, 0.0698, 0.3578],\n",
      "        [0.5649, 0.0830, 0.3521],\n",
      "        [0.5261, 0.1249, 0.3490],\n",
      "        [0.4138, 0.1049, 0.4813],\n",
      "        [0.4782, 0.0746, 0.4473],\n",
      "        [0.6002, 0.0703, 0.3295],\n",
      "        [0.5066, 0.1103, 0.3831],\n",
      "        [0.4537, 0.0980, 0.4483],\n",
      "        [0.3032, 0.3470, 0.3497]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Pass the data through the network once without backpropagation and without softmax activation\n",
    "model_with_softmax = nn.Sequential(\n",
    "    nn.Linear(features.shape[1], len(np.unique(labels))),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "outputs_with_softmax = model_with_softmax(features)\n",
    "print(outputs_with_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfefd7",
   "metadata": {},
   "source": [
    "A softmax activation funciton takes a vector of real numbers as input and normalizes it into a probability distribution that sums up to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd905157",
   "metadata": {},
   "source": [
    "**(b) Divide your data into 3-fold training and testing groups, within each fold further divide your\n",
    "training data into 80% training and 20% validation, choose the model for the epoch with lowest validation\n",
    "error. Report error in terms of success rate of classification. How well is the prediction?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "57ccb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wine, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(13, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "#         output += 1 # shift output range from 0-2 to 1-3\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e0cb9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model,train_X,train_y,epochs,draw_curve=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    model: a PyTorch model\n",
    "    train_X: np.array shape(ndata,nfeatures)\n",
    "    train_y: np.array shape(ndata)\n",
    "    epochs: int\n",
    "    draw_curve: bool\n",
    "    \"\"\"\n",
    "    ### Define your loss function, optimizer. Convert data to torch tensor ###\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_y = train_y - 1\n",
    "    \n",
    "    # Convert the data to PyTorch tensors\n",
    "    Xs = torch.tensor(train_X).float()\n",
    "    ys = torch.tensor(train_y).long()\n",
    "    \n",
    "    # Kfolds \n",
    "    kf = KFold(n_splits=3,shuffle=True)\n",
    "    for train_index, test_index in kf.split(Xs):\n",
    "        train_X=Xs[train_index]\n",
    "        train_y=ys[train_index]\n",
    "        test_X=Xs[test_index]\n",
    "        test_y=ys[test_index]\n",
    "    \n",
    "    ### Split training examples further into training and validation ###\n",
    "    train_X,val_X,train_y,val_y=train_test_split(train_X,train_y, test_size = 0.20)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     val_X = torch.tensor(val_X).float()\n",
    "#     val_y = torch.tensor(val_y).long()\n",
    "\n",
    "    \n",
    "    \n",
    "    val_array=[]\n",
    "    lowest_val_loss = np.inf\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        ### Compute the loss and do backpropagation ###\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        train_out = model(train_X)\n",
    "        train_loss = loss(train_out, train_y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### compute validation loss and keep track of the lowest val loss ###\n",
    "        \n",
    "\n",
    "        # compute validation loss\n",
    "        val_out = model(val_X) \n",
    "        val_loss = loss(val_out, val_y)\n",
    "        \n",
    "        \n",
    "         # append val loss to val_array\n",
    "        val_array.append(val_loss.item())\n",
    "\n",
    "        # keep track of the lowest val loss\n",
    "        if val_loss < lowest_val_loss:\n",
    "            lowest_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "  \n",
    "     # The final number of epochs is when the minimum error in validation set occurs    \n",
    "   \n",
    "    final_epochs=np.argmin(val_array)+1\n",
    "    print(\"Number of epochs with lowest validation:\",final_epochs)\n",
    "    ### Recover the model weight ###\n",
    "    model.load_state_dict(torch.load('model.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    ### Plot the validation loss curve ###\n",
    "\n",
    "    if draw_curve:\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(val_array))+1,val_array,label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "33f3c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs with lowest validation: 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_sample = Wine()\n",
    "train_and_val(wine_sample, df_norm.iloc[:,:].values, df.iloc[:,-1].values, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c938d6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f76556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcc2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
